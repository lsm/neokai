# CI - Build, test, and quality checks
#
# On PRs to dev:
# 1. Code quality checks + unit/integration tests (parallel)
# 2. Build, binary compilation, and E2E tests are SKIPPED for faster feedback
#
# On push to dev (after merge) and all main branch activity:
# 1. Code quality checks + unit/integration tests (parallel)
# 2. Build linux-x64 binary + smoke test (after tests pass)
# 3. Run full E2E test matrix against the binary
# 4. "All Tests Pass" gate (used by release.yml to verify CI status)
#
# Release is handled separately in release.yml (triggered by version tags).
# Unit test coverage (daemon, web) is uploaded to Codecov.

name: CI

on:
  push:
    branches: [main, dev]
  pull_request:
    branches: [main, dev]
  workflow_dispatch:

permissions:
  contents: read

jobs:
  # ============================================
  # CODE QUALITY CHECKS
  # ============================================

  check:
    name: Lint, Knip, Format & Type Check
    runs-on: ubuntu-latest
    timeout-minutes: 3

    steps:
      - uses: actions/checkout@v4

      - name: Setup Bun
        uses: ./.github/actions/setup-bun
        with:
          bun-version: 1.3.6

      - name: Install dependencies
        run: bun install

      - name: Run linter
        id: lint
        run: bun run lint
        continue-on-error: true

      - name: Check formatting
        id: format
        run: bun run format:check
        continue-on-error: true

      - name: Run Knip
        id: knip
        run: bun run knip
        continue-on-error: true

      - name: Run type check
        id: typecheck
        run: bun run typecheck
        continue-on-error: true

      - name: Check for failures
        if: steps.lint.outcome == 'failure' || steps.format.outcome == 'failure' || steps.knip.outcome == 'failure' || steps.typecheck.outcome == 'failure'
        run: |
          echo "One or more checks failed:"
          echo "  Lint: ${{ steps.lint.outcome }}"
          echo "  Format: ${{ steps.format.outcome }}"
          echo "  Knip: ${{ steps.knip.outcome }}"
          echo "  Typecheck: ${{ steps.typecheck.outcome }}"
          exit 1

  # ============================================
  # DAEMON TESTS (Matrix by module for parallelization)
  # ============================================

  test-daemon-offline:
    name: Daemon Offline (${{ matrix.module }})
    runs-on: ubuntu-latest
    timeout-minutes: 5

    strategy:
      fail-fast: false
      matrix:
        module:
          - agent
          - session
          - rpc
          - database
          - filesystem
          - websocket
          - git
          - components
          - mcp
        include:
          - module: agent
            test_path: tests/integration/agent
          - module: session
            test_path: tests/integration/session
          - module: rpc
            test_path: tests/integration/rpc
          - module: database
            test_path: tests/integration/database
          - module: filesystem
            test_path: tests/integration/filesystem
          - module: websocket
            test_path: tests/integration/websocket
          - module: git
            test_path: tests/integration/git
          - module: components
            test_path: tests/integration/components
          - module: mcp
            test_path: tests/integration/mcp

    steps:
      - uses: actions/checkout@v4

      - name: Setup Bun
        uses: ./.github/actions/setup-bun
        with:
          bun-version: 1.3.6

      - name: Install dependencies
        run: bun install

      - name: Run daemon tests (${{ matrix.module }})
        working-directory: packages/daemon
        run: bun test ${{ matrix.test_path }}
        env:
          GLM_API_KEY: ""
          CLAUDE_CODE_OAUTH_TOKEN: ""

  # ============================================
  # DAEMON ONLINE TESTS (Require API credentials)
  # ============================================

  test-daemon-online:
    name: Daemon Online (${{ matrix.module }})
    runs-on: ubuntu-latest
    if: github.ref_type != 'tag'
    timeout-minutes: ${{ matrix.timeout || 5 }}

    strategy:
      fail-fast: false
      matrix:
        module:
          - agent
          - components
          - coordinator
          - convo
          - features
          - glm
          - lifecycle
          - mcp
          - providers
          - rewind
          - rpc
          - sdk
        include:
          - module: components
            test_path: tests/online/components
          - module: glm
            test_path: tests/online/glm
          - module: mcp
            test_path: tests/online/mcp
          - module: providers
            test_path: tests/online/providers
          - module: rpc
            test_path: tests/online/rpc
          - module: sdk
            test_path: tests/online/sdk
          - module: agent
            test_path: tests/online/agent
          - module: coordinator
            test_path: tests/online/coordinator
            timeout: 10
          - module: convo
            test_path: tests/online/convo
          - module: features
            test_path: tests/online/features
          - module: lifecycle
            test_path: tests/online/lifecycle
          - module: rewind
            test_path: tests/online/rewind

    steps:
      - uses: actions/checkout@v4

      - name: Setup Bun
        uses: ./.github/actions/setup-bun
        with:
          bun-version: 1.3.6

      - name: Install dependencies
        run: bun install

      - name: Run daemon online tests (${{ matrix.module }})
        working-directory: packages/daemon
        run: bun test ${{ matrix.test_path }}
        env:
          GLM_API_KEY: ${{ secrets.GLM_API_KEY }}
          CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}

  # ============================================
  # DAEMON + SHARED UNIT TESTS (For coverage measurement)
  # ============================================

  test-daemon-shared-unit:
    name: Daemon + Shared Unit Tests (Coverage)
    runs-on: ubuntu-latest
    timeout-minutes: 3

    steps:
      - uses: actions/checkout@v4

      - name: Setup Bun
        uses: ./.github/actions/setup-bun
        with:
          bun-version: 1.3.6

      - name: Install dependencies
        run: bun install

      - name: Run daemon + shared unit tests from root
        run: bun test packages/daemon/tests/unit packages/shared/tests --coverage --coverage-reporter=lcov --coverage-dir=coverage
        env:
          GLM_API_KEY: ""
          CLAUDE_CODE_OAUTH_TOKEN: ""

      - name: Upload to Codecov
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: coverage/lcov.info
          flags: daemon
          fail_ci_if_error: false

  # ============================================
  # WEB TESTS
  # ============================================

  test-web:
    name: Web Tests
    runs-on: ubuntu-latest
    timeout-minutes: 3

    steps:
      - uses: actions/checkout@v4

      - name: Setup Bun
        uses: ./.github/actions/setup-bun
        with:
          bun-version: 1.3.6

      - name: Install dependencies
        run: bun install

      - name: Run web tests
        working-directory: packages/web
        run: bun run coverage

      - name: Fix coverage paths for monorepo
        run: |
          # Vitest generates paths relative to package dir (src/...)
          # Codecov needs paths relative to repo root (packages/web/src/...)
          sed -i 's|^SF:src/|SF:packages/web/src/|g' packages/web/coverage/lcov.info
          # Fix cross-package paths (../shared/src/... -> packages/shared/src/...)
          sed -i 's|^SF:\.\./shared/|SF:packages/shared/|g' packages/web/coverage/lcov.info

      - name: Upload to Codecov
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: packages/web/coverage/lcov.info
          flags: web
          fail_ci_if_error: false

  # ============================================
  # CLI TESTS
  # ============================================

  test-cli:
    name: CLI Tests
    runs-on: ubuntu-latest
    timeout-minutes: 2

    steps:
      - uses: actions/checkout@v4

      - name: Setup Bun
        uses: ./.github/actions/setup-bun
        with:
          bun-version: 1.3.6

      - name: Install dependencies
        run: bun install

      - name: Run CLI tests
        working-directory: packages/cli
        run: bun test

  # ============================================
  # BUILD - Compile linux-x64 binary + smoke test
  # Runs after all unit/integration tests pass
  # ============================================

  build:
    name: Build Binary (linux-x64)
    runs-on: ubuntu-latest
    needs: [check, test-daemon-offline, test-daemon-online, test-daemon-shared-unit, test-web, test-cli]
    if: github.event_name == 'push' || github.base_ref == 'main'
    timeout-minutes: 10

    steps:
      - uses: actions/checkout@v4

      - name: Setup Bun
        uses: ./.github/actions/setup-bun
        with:
          bun-version: 1.3.6

      - name: Install dependencies
        run: bun install

      - name: Build web frontend
        run: cd packages/web && bun run build

      - name: Generate embedded assets
        run: bun run scripts/generate-embedded-assets.ts

      - name: Compile binary
        run: bun build --compile --target=bun-linux-x64 --outfile=dist/bin/kai-linux-x64 packages/cli/prod-entry.ts

      - name: Smoke test
        run: bun run scripts/smoke-test.ts ./dist/bin/kai-linux-x64
        env:
          GLM_API_KEY: smoke-test

      - name: Upload binary artifact
        uses: actions/upload-artifact@v4
        with:
          name: kai-linux-x64-e2e
          path: dist/bin/kai-linux-x64

  # ============================================
  # DISCOVER TESTS
  # Auto-discover and categorize E2E test files
  # Tests are split into two groups:
  #   - no-llm: UI-only tests (run fully parallel)
  #   - llm: Tests requiring LLM round-trips (max 4 parallel)
  # ============================================

  discover:
    name: Discover Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.base_ref == 'main'
    outputs:
      tests_no_llm: ${{ steps.categorize.outputs.tests_no_llm }}
      tests_llm: ${{ steps.categorize.outputs.tests_llm }}
    steps:
      - uses: actions/checkout@v4

      - name: Categorize E2E test files
        id: categorize
        run: |
          # Tests that require LLM API round-trips (send messages, wait for agent responses)
          LLM_TESTS=(
            2-stage-creation
            auto-title-generation
            auto-scroll-toggle
            chat-flow
            chat-flow-improved
            context-dropdown
            context-usage-display
            context-usage-dropdown-content
            file-attachment-send
            file-operations
            interrupt-button
            interrupt-error-bug
            message-input-processing-state
            message-pagination
            message-removal
            message-send-receive
            page-refresh-context
            page-refresh-session-state
            processing-state
            question-form-persistence
            slash-cmd-built-in
            slash-cmd-edge-cases
            slash-cmd-navigation
            slash-cmd-selection
          )

          # Tests to exclude entirely (disabled, pending refactor)
          EXCLUDED_TESTS=(
            model-selection-persistence
            rewind-modal
          )

          # Build lookup sets
          declare -A LLM_SET EXCLUDED_SET
          for t in "${LLM_TESTS[@]}"; do LLM_SET[$t]=1; done
          for t in "${EXCLUDED_TESTS[@]}"; do EXCLUDED_SET[$t]=1; done

          # Find all test files and categorize
          NO_LLM=()
          LLM=()
          for file in $(find packages/e2e/tests -maxdepth 1 -name "*.e2e.ts" -type f | xargs -I{} basename {} .e2e.ts | sort); do
            if [[ -n "${EXCLUDED_SET[$file]}" ]]; then
              echo "EXCLUDED: $file"
              continue
            fi
            if [[ -n "${LLM_SET[$file]}" ]]; then
              LLM+=("$file")
            else
              NO_LLM+=("$file")
            fi
          done

          # Convert to JSON arrays
          tests_no_llm=$(printf '%s\n' "${NO_LLM[@]}" | jq -R -s -c 'split("\n") | map(select(length > 0))')
          tests_llm=$(printf '%s\n' "${LLM[@]}" | jq -R -s -c 'split("\n") | map(select(length > 0))')

          echo "No-LLM tests (${#NO_LLM[@]}): $tests_no_llm"
          echo "LLM tests (${#LLM[@]}): $tests_llm"
          echo "Excluded (${#EXCLUDED_TESTS[@]}): ${EXCLUDED_TESTS[*]}"

          echo "tests_no_llm=$tests_no_llm" >> $GITHUB_OUTPUT
          echo "tests_llm=$tests_llm" >> $GITHUB_OUTPUT

  # ============================================
  # E2E TESTS - No LLM (Matrix, fully parallel)
  # UI-only tests that don't require LLM API calls
  # ============================================

  e2e:
    name: E2E (${{ matrix.test }})
    runs-on: ubuntu-latest
    needs: [build, discover]
    if: github.event_name == 'push' || github.base_ref == 'main'
    timeout-minutes: 10
    strategy:
      fail-fast: false
      matrix:
        test: ${{ fromJson(needs.discover.outputs.tests_no_llm) }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup Bun
        uses: ./.github/actions/setup-bun
        with:
          bun-version: 1.3.6

      - name: Install dependencies
        run: |
          echo "Starting bun install at $(date)"
          timeout 180 bun install --verbose 2>&1 || {
            echo "bun install timed out or failed"
            exit 1
          }
          echo "Finished bun install at $(date)"

      - name: Install Playwright browsers
        run: cd packages/e2e && bunx playwright install chromium

      - name: Download binary
        uses: actions/download-artifact@v4
        with:
          name: kai-linux-x64-e2e
          path: dist/bin/

      - name: Run E2E test against binary (${{ matrix.test }})
        run: |
          chmod +x dist/bin/kai-linux-x64
          WORKSPACE=$(mktemp -d)
          PORT=$(python3 -c 'import socket; s=socket.socket(); s.bind(("",0)); print(s.getsockname()[1]); s.close()')
          echo "Starting binary on port $PORT with workspace $WORKSPACE"

          ./dist/bin/kai-linux-x64 --port "$PORT" "$WORKSPACE" &
          BINARY_PID=$!
          trap 'kill $BINARY_PID 2>/dev/null; rm -rf "$WORKSPACE"' EXIT

          for i in $(seq 1 60); do
            if curl -s "http://localhost:$PORT/" > /dev/null 2>&1; then
              echo "Server ready on port $PORT (PID $BINARY_PID)"
              break
            fi
            if ! kill -0 $BINARY_PID 2>/dev/null; then
              echo "Binary process died unexpectedly"
              exit 1
            fi
            sleep 0.5
          done

          if ! curl -s "http://localhost:$PORT/" > /dev/null 2>&1; then
            echo "Server failed to start within 30s"
            exit 1
          fi

          export PLAYWRIGHT_BASE_URL="http://localhost:$PORT"
          cd packages/e2e
          xvfb-run --auto-servernum --server-args="-screen 0 1920x1080x24" \
            bunx playwright test "tests/${{ matrix.test }}.e2e.ts"
        env:
          GLM_API_KEY: ${{ secrets.GLM_API_KEY }}
          DEFAULT_MODEL: sonnet
          CI: true

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: e2e-results-${{ matrix.test }}
          path: |
            packages/e2e/test-results/
            packages/e2e/playwright-report/
          retention-days: 7

  # ============================================
  # E2E TESTS - LLM Required (Matrix, max 4 parallel)
  # Tests that send messages and wait for LLM responses
  # ============================================

  e2e-llm:
    name: E2E LLM (${{ matrix.test }})
    runs-on: ubuntu-latest
    needs: [build, discover]
    if: github.event_name == 'push' || github.base_ref == 'main'
    timeout-minutes: 10
    strategy:
      fail-fast: false
      max-parallel: 4
      matrix:
        test: ${{ fromJson(needs.discover.outputs.tests_llm) }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup Bun
        uses: ./.github/actions/setup-bun
        with:
          bun-version: 1.3.6

      - name: Install dependencies
        run: |
          echo "Starting bun install at $(date)"
          timeout 180 bun install --verbose 2>&1 || {
            echo "bun install timed out or failed"
            exit 1
          }
          echo "Finished bun install at $(date)"

      - name: Install Playwright browsers
        run: cd packages/e2e && bunx playwright install chromium

      - name: Download binary
        uses: actions/download-artifact@v4
        with:
          name: kai-linux-x64-e2e
          path: dist/bin/

      - name: Run E2E test against binary (${{ matrix.test }})
        run: |
          chmod +x dist/bin/kai-linux-x64
          WORKSPACE=$(mktemp -d)
          PORT=$(python3 -c 'import socket; s=socket.socket(); s.bind(("",0)); print(s.getsockname()[1]); s.close()')
          echo "Starting binary on port $PORT with workspace $WORKSPACE"

          ./dist/bin/kai-linux-x64 --port "$PORT" "$WORKSPACE" &
          BINARY_PID=$!
          trap 'kill $BINARY_PID 2>/dev/null; rm -rf "$WORKSPACE"' EXIT

          for i in $(seq 1 60); do
            if curl -s "http://localhost:$PORT/" > /dev/null 2>&1; then
              echo "Server ready on port $PORT (PID $BINARY_PID)"
              break
            fi
            if ! kill -0 $BINARY_PID 2>/dev/null; then
              echo "Binary process died unexpectedly"
              exit 1
            fi
            sleep 0.5
          done

          if ! curl -s "http://localhost:$PORT/" > /dev/null 2>&1; then
            echo "Server failed to start within 30s"
            exit 1
          fi

          export PLAYWRIGHT_BASE_URL="http://localhost:$PORT"
          cd packages/e2e
          xvfb-run --auto-servernum --server-args="-screen 0 1920x1080x24" \
            bunx playwright test "tests/${{ matrix.test }}.e2e.ts"
        env:
          GLM_API_KEY: ${{ secrets.GLM_API_KEY }}
          DEFAULT_MODEL: sonnet
          CI: true

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: e2e-results-${{ matrix.test }}
          path: |
            packages/e2e/test-results/
            packages/e2e/playwright-report/
          retention-days: 7

  # ============================================
  # GATE - All tests (unit, integration, E2E) must pass
  # ============================================

  all-tests-pass:
    name: All Tests Pass
    runs-on: ubuntu-latest
    timeout-minutes: 1
    if: always()
    needs:
      - check
      - test-daemon-offline
      - test-daemon-online
      - test-daemon-shared-unit
      - test-web
      - test-cli
      - e2e
      - e2e-llm

    steps:
      - name: Check all jobs passed
        run: |
          if [[ "${{ needs.check.result }}" != "success" ]] || \
             [[ "${{ needs.test-daemon-offline.result }}" != "success" ]] || \
             [[ "${{ needs.test-daemon-online.result }}" != "success" ]] || \
             [[ "${{ needs.test-daemon-shared-unit.result }}" != "success" ]] || \
             [[ "${{ needs.test-web.result }}" != "success" ]] || \
             [[ "${{ needs.test-cli.result }}" != "success" ]] || \
             [[ "${{ needs.e2e.result }}" != "success" && "${{ needs.e2e.result }}" != "skipped" ]] || \
             [[ "${{ needs.e2e-llm.result }}" != "success" && "${{ needs.e2e-llm.result }}" != "skipped" ]]; then
            echo "One or more jobs failed:"
            echo "  check: ${{ needs.check.result }}"
            echo "  test-daemon-offline: ${{ needs.test-daemon-offline.result }}"
            echo "  test-daemon-online: ${{ needs.test-daemon-online.result }}"
            echo "  test-daemon-shared-unit: ${{ needs.test-daemon-shared-unit.result }}"
            echo "  test-web: ${{ needs.test-web.result }}"
            echo "  test-cli: ${{ needs.test-cli.result }}"
            echo "  e2e: ${{ needs.e2e.result }}"
            echo "  e2e-llm: ${{ needs.e2e-llm.result }}"
            exit 1
          fi
          echo "All tests passed!"
